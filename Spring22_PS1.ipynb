{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file to your computer (use \"Raw\" link on gist\\github), run Anaconda and choose this file in Anaconda's file explorer. Use the `Python 3` version. Everything that follows assumes that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/).\n",
    "\n",
    "To run code in a cell or to render [Markdown](https://en.wikipedia.org/wiki/Markdown)+[LaTeX](https://en.wikipedia.org/wiki/LaTeX) press `Ctr+Enter` or `[>|]`(like \"play\") button above. To edit any code or text cell [double]click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above.\n",
    "\n",
    "If certain output is given for some cells, that means that you are expected to get similar results.\n",
    "\n",
    "Total: 80 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Algebra\n",
    "**1\\.1 [4pt]**  Given three square matrices $Q, R, S \\in R^{n√ón}$ , which statements are true in general?\n",
    "\n",
    "(a) $(QRS)^{-1} = S^{-1}R^{‚àí1}Q^{‚àí1}$\n",
    "\n",
    "(b) $QR = RQ$\n",
    "\n",
    "(c) $(QRS)^T = Q^T R^T S^T$\n",
    "\n",
    "(d) $Q(R + S) = QS + QR$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "       d\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2 Vectors** \n",
    "\n",
    "**1\\.2\\.1 [1pt]** Given points $p_1 = (1, 6, 5)$ and $p_2 = (‚àí2, 2, 5)$, solve for $v_1$ the vector from $p_1$ to $p_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "           (-3,-4,0) \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2\\.2 [1pt]** Given a third point $p_3 = (0, 6, 5)$, solve for $v_2$ the vector from $p_1$ to $p_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "            (-1,0,0)\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2\\.3 [1pt]** Find the value for the magnitude of $v_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "            5\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2\\.4 [1pt]**  Solve for the scalar (dot) product $v_2 \\cdot v_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "           3\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2.5 [1pt]** If two vectors $u, v \\in R^n$ are orthogonal, what is the value of their scalar (dot) product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "            0\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Partial Derivatives\n",
    "**2\\.1 [6pt]** Compute the partial derivative of $f$ with respect to each of: $x$, $y$, and $z$.\n",
    "$$f\\left( {x,y,z} \\right) = 4{x^3}{y^2} - {{\\bf{e}}^z}{y^4} + \\frac{{{z^3}}}{{{x^2}}} + 4y - {x^{16}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$\\delta f / \\delta x = 12x^2y^2 - 2\\frac{z^3}{x^3} - 16x^15 $\n",
    "\n",
    "$\\delta f / \\delta y = 8x^3y- 4e^zy^3 + 4$\n",
    "\n",
    "$\\delta f / \\delta z = -e^zy^4 + \\frac{3z^2}{x^2}$\n",
    "\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2\\.2 [2pt]** Using the chain rule, compute $ùëì'(\\theta)$ (or $\\delta f / \\delta \\theta$), where $f$ is composed of chained functions $ùëì(\\theta) = ùëì_1(ùëì_2(\\theta))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "            <br> $ f^{'}(\\theta) = f_1^{'}(f_2(\\theta))\\cdotp f^{'}_2(\\theta) $\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2\\.3 [4pt]** Using the chain rule, compute the partial derivatives of $ùëì(\\theta_1, \\theta_2) = ùëì_1(ùëì_2(\\theta_1, \\theta_2))$ with respect to each of: $\\theta_1$ and $\\theta_2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$ \\delta f(\\theta_1, \\theta_2) / \\delta \\theta_1 =  \\frac{ \\delta f_1(f_2(\\theta_1, \\theta_2))}{\\delta \\theta_1}\\cdotp \\frac { \\delta f_2(\\theta_1, \\theta_2)}{\\delta \\theta_1} $\n",
    "\n",
    "$ \\delta f(\\theta_1, \\theta_2) / \\delta \\theta_2 =  \\frac{ \\delta f_1(f_2(\\theta_1, \\theta_2))}{\\delta \\theta_2}\\cdotp \\frac { \\delta f_2(\\theta_1, \\theta_2)}{\\delta \\theta_2} $\n",
    "\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Descent\n",
    "With each step of gradient descent, your parameter $\\theta_j$ comes closer to the local minimum of the cost $J(Œ∏)$.\n",
    "\n",
    "**3\\.1 [10pt]**\n",
    "Compute the partial derivative of the regression loss function \n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big)^2$$ \n",
    "with respect to the parameter $\\theta_j$, where the hypothesis $h(x;\\theta)$ is given by the linear model \n",
    "$$ h(x;\\theta) = \\theta^T x$$\n",
    "\n",
    "Please show all your steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "           $$ \\frac{\\delta J(\\theta)}{\\delta \\theta_j} \\\\ = \\frac{1}{m} \\sum^{m}_{i =1} (h(x^{(i)};\\theta) - y^{(i)}) \\frac{\\delta h(x^{(i)};\\theta)}{\\delta \\theta_j} \\\\ = \\frac{1}{m} \\sum^{m}_{i =1} (h(x^{(i)};\\theta) - y^{(i)}) x^{(i)}_j \\\\ = \\frac{1}{m} \\sum^{m}_{i =1} (\\theta^Tx^{(i)} - y^{(i)}) x^{(i)}_j\n",
    "           $$ \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.2 [10pt]**\n",
    "Write a mathematical formulation of how gradient descent will adjust $\\theta_j$ values to minimize the cost $J(Œ∏)$. Then, describe each term in the formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "           $\\\\ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum^{m}_{i=1}(h(x^{(i)};\\theta) - y^{(i)}) x^{(i)}_j $\n",
    "           <br> The $ \\theta_j $ in the left of the equation is the $\\theta_j$ we will update in this iteration. \n",
    "           <br>The $ \\theta_j $ in the right of the equation is the $\\theta_j$ we get from the privious iteration.\n",
    "           <br>$\\alpha$ is the learning rate.\n",
    "           <br>$\\frac{1}{m} \\sum^{m}_{i =1} (h(x^{(i)};\\theta) - y^{(i)}) x^{(i)}_j$ is the gradient which is the partial derivate of our loss function\n",
    "           \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.3 [10pt]**\n",
    "If $x^{(i)} \\in R^4$, and $m=3$, draw the tensors $x$, $\\theta$, $h$, in $ h(x;\\theta) = \\theta^T x$, together with the ground-truth tensor $y$, specifying the dimensions of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "            <br> $ x^{(i)} = \\begin{bmatrix} 1 \\ \\ \\ \\ \\  1  \\ \\ \\ \\ \\ 1 \\\\ x_1^{(1)} \\ x_1^{(2)} \\ x_1^{(3)} \\\\ x_2^{(1)} \\ x_2^{(2)} \\ x_2^{(3)} \\\\ x_3^{(1)} \\ x_3^{(2)} \\ x_3^{(3)} \\\\ x_4^{(1)} \\ x_4^{(2)} \\ x_4^{(3)} \\end{bmatrix}_{ \\ 5 \\times 3 }$\n",
    "            <br> $\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\theta_3 \\\\ \\theta_4 \\end{bmatrix}_{\\ 5 \\times 1}$\n",
    "            <br>$ h(x, \\theta) = \\begin{bmatrix} \\theta_0 \\ \\theta_1 \\ \\theta_2 \\ \\theta_3 \\ \\theta_4 \\end{bmatrix}_{1 \\times 5} \\begin{bmatrix} 1 \\ \\ \\ \\ \\  1  \\ \\ \\ \\ \\ 1 \\\\ x_1^{(1)} \\ x_1^{(2)} \\ x_1^{(3)} \\\\ x_2^{(1)} \\ x_2^{(2)} \\ x_2^{(3)} \\\\ x_3^{(1)} \\ x_3^{(2)} \\ x_3^{(3)} \\\\ x_4^{(1)} \\ x_4^{(2)} \\ x_4^{(3)} \\end{bmatrix}_{ \\ 5 \\times 3 } = \\begin{bmatrix} \\sum^4_{j=0}\\theta_j x_j^{(1)} \\ \\ \\sum^4_{j=0}\\theta_j x_j^{(2)}  \\ \\ \\sum^4_{j=0}\\theta_j x_j^{(3)} \\end{bmatrix}_{\\ 1 \\times 3} $ where $x_0^{(i)} = 1 $ for $ i \\in {1,2,3} $\n",
    "            <br> $ y = \\begin{bmatrix} y_1 \\ y_2   \\ y_3 \\end{bmatrix}_{ \\ 1 \\times 3 } $\n",
    "         \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.4 [2pt]**\n",
    "How do we know that gradient descent has converged?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "When the each of the parameters does not change any more or the loss function does not change\n",
    "\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.5 [6pt]**\n",
    "Write an L2 regularized form of $$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big)^2,$$ explain the additional term(s), and explain why the additional term(s) achieve regularization (i.e. help avoid overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "            $$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big)^2 + \\lambda \\sum^n_{j=1} \\theta^2_j $$\n",
    "             When the overfitting happens, there are two characterisitcs. First, the coefficients of the variables will be extremly large. Second, the training error will close to 0. In the L2 regularization, we add a penelty term $ \\lambda \\sum^n_{j=1} \\theta^2_j $ to restrict the magnitude of the coefficients and therefore help avoid overfitting. \n",
    "             \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cost Function Design for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a classification problem with inputs $x$ and corresponding labels $y$, where $y^{(i)} \\in \\{-1,+1\\}$. We would like to learn a classifier that computes a linear function of the input $h=\\theta^T x$, and predicts $+1$ if $h(x^{(i)}) \\ge 0$, or $-1$ otherwise. \n",
    "\n",
    "**4\\.1 [2pt]**\n",
    "What can be said about the correctness of the classifier‚Äôs prediction if $y^{(i)}h(x^{(i)})>0$? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "         <br>  If our prediction is larger than 0, we will classify it to +1. When the ground true label is 1, we will get $y^{(i)}h(x^{(i)})>0$. If our prediction is less than 0, we will classify it to -1. If the groud true label is -1, we will also get $y^{(i)}h(x^{(i)})>0$. In this way, $y^{(i)}h(x^{(i)})>0$ is a measurement of the correctness of our prediction.\n",
    "           \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4\\.2 [2pt]**\n",
    "What if $y^{(i)}h(x^{(i)})<0$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "            <br>When $y^{(i)}h(x^{(i)})< 0$, our prediction is classified as -1 while our ground true label is 1 or our prediction is classified as 1 while our ground true label is -1.  $y^{(i)}h(x^{(i)})< 0$ mesures the error rate of our prediction.\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4\\.3 [2pt]**\n",
    "Is $yh(x)$ a good loss function to minimize? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "<br> No, based on the analysis before,  $yh(x)$ is larger than 0 when the model has the correct classification. Therefore, in order to get the best prediction results, we should maximize the $yh(x)$ instead of minimizing. Even though we try to maximize $yh(x)$, it will not guarantee improving our prediction. For example, we might have extremely one point with large $h(x)$ and has been classified into the correct class while other data with small magnitude of $h(x)$ have been classified into the wrong place. That situation can give us a huge $yh(x)$ but low prediction accuracy.\n",
    "           \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Numpy\n",
    "**5\\.1 [5pt]**\n",
    "Modify the cell below to return a 5x5 matrix of ones. Put some code there and press `Ctrl+Enter` to execute contents of the cell. You should see something like the output below. [[1]](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.creation.html#arrays-creation) [[2]](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.array-creation.html#routines-array-creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# raise NotImplementedError(\"Replace this raise statement with the code \"\n",
    "                         # \"that prints 5x5 matrix of ones\")\n",
    "a = np.ones((5,5))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 [10pt]** \n",
    "Vectorizing your code is very important to get results in a reasonable time. Let A be a 10x10 matrix and x be a 10-element column vector. Your friend writes the following code. How would you vectorize this code to run without any for loops? Compare execution speed for different values of `n` with [`%timeit`](http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.06296563]\n",
      " [1.54113845]\n",
      " [1.71032661]\n",
      " [1.95643173]\n",
      " [1.88641284]\n",
      " [1.72936105]\n",
      " [1.59562938]\n",
      " [1.87410391]\n",
      " [1.52478666]\n",
      " [1.95268221]]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "def compute_something(A, x):\n",
    "    v = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            v[i] += A[i, j] * x[j]\n",
    "    return v\n",
    "            \n",
    "A = np.random.rand(n, n)\n",
    "x = np.random.rand(n, 1)\n",
    "print(compute_something(A, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.06296563]\n",
      " [1.54113845]\n",
      " [1.71032661]\n",
      " [1.95643173]\n",
      " [1.88641284]\n",
      " [1.72936105]\n",
      " [1.59562938]\n",
      " [1.87410391]\n",
      " [1.52478666]\n",
      " [1.95268221]]\n"
     ]
    }
   ],
   "source": [
    "def vectorized(A, x):\n",
    "    return np.sum(np.multiply(A, x.T), axis = 1).reshape(n,1)\n",
    "    \n",
    "print(vectorized(A, x))\n",
    "assert np.max(abs(vectorized(A, x) - compute_something(A, x))) < 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 ¬µs ¬± 81.4 ¬µs per loop (mean ¬± std. dev. of 7 runs, 5 loops each)\n",
      "11.5 ¬µs ¬± 6.24 ¬µs per loop (mean ¬± std. dev. of 7 runs, 5 loops each)\n",
      "---\n",
      "419 ¬µs ¬± 21.7 ¬µs per loop (mean ¬± std. dev. of 7 runs, 5 loops each)\n",
      "15 ¬µs ¬± 8.13 ¬µs per loop (mean ¬± std. dev. of 7 runs, 5 loops each)\n",
      "---\n",
      "36.3 ms ¬± 313 ¬µs per loop (mean ¬± std. dev. of 7 runs, 5 loops each)\n",
      "The slowest run took 8.01 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "37.4 ¬µs ¬± 45.6 ¬µs per loop (mean ¬± std. dev. of 7 runs, 5 loops each)\n",
      "---\n",
      "914 ms ¬± 4.9 ms per loop (mean ¬± std. dev. of 7 runs, 5 loops each)\n",
      "316 ¬µs ¬± 165 ¬µs per loop (mean ¬± std. dev. of 7 runs, 5 loops each)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 10, 100, 500]:\n",
    "    A = np.random.rand(n, n)\n",
    "    x = np.random.rand(n, 1)\n",
    "    %timeit -n 5 compute_something(A, x)\n",
    "    %timeit -n 5 vectorized(A, x)\n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
